Active Safety Systems or Advanced Driver Assistance Systems (ADAS) are usually composed by an acquisition system to get some information from different sensors, a processing phase to deliver warnings to the driver and sometimes a react phase to control the acceleration, braking, etc. Using both proprioceptive sensors (e.g. Inertial measurement unit, GPS) and exteroceptive sensors (LIDAR, RADAR) is common in ADAS systems even if most intelligent vehicles (e.g. The Google Driverless Car) use LIDAR mainly since it can generate a high resolution 3D map of the environment. As sensors provides heterogeneous measurements, data fusion from different sensors is required to achieve different tasks in ADAS systems.

Performed vehicle localization and obstacle detection using data fusion from both proprioceptive and exteroceptive sensors. Along with the development in image processing, cameras are increasingly supplemented by other sensing technologies to expand ADAS capabilities and also to enhance the robustness of results. Precise road estimation algorithm is based on measurements provided from camera, radar, wheel speed sensors, and IMU. where camera is used for detecting lane marking. As plenty of information can be exploited from images (visual attributes, motion, disparity), moreover cameras are really inexpensive compared with LIDAR, we do believe that, vision-based ADAS system has a promising future. In fact, many researches have already proved abilities of purely vision-based systems. For example, stereo vision is used now to describe the environment to detect obstacles by computing occupancy grids. The road, as an important source of information could be detected through feature extraction and classification.

Motion estimation is required for many tasks in the ADAS systems such as egomotion estimation, moving-object detection, and 3-D reconstruction while optical flow shows its potential. Many research papers have studies the influence of vehicle speed and scene texture on optical-flow accuracy using synthetic images. However, optical flow is rarely implemented on intelligent vehicles. It is usually combined with other sensors like radars or stereovision and plays consequently just a supporting role. The main reason is that existing optical flow estimation results are not always convincing, especially when vehicle motion is large and scene textures are rare. (The LIDAR system mounted on the google driverless car costs $70.000. The camera hardware can cost just tens of dollars)
An important issue of intelligent vehicles is to detect and identify from images relevant objects in the driving environment (obstacles on the road for instance) for ensuring navigation in safe conditions as explained before, stereo-vision based systems are widely used to achieve this objective. However, the resulting disparity map require a pre-processing step of calibration and image rectification. We consider that stereovision is not always necessary as it is possible to exploit first monocular vision and motion analysis. 

This thesis deals with monocular vision for object detection in the context of automatic driver assistance systems. Our proposed method uses 2D motion information (optical flow and Focus of Expansion) to define and to exploit specific voting spaces in order to detect objects Optical flow estimation and FOE estimation are considered as two preliminary steps. The success of these two steps conditions the quality of detection. Hence, we have to deal with two difficulties that are specific to the applications: homogeneous regions and large displacements. These difficulties are both problematic for estimating optical flow with enough precision. In this thesis, we propose a model-based motion estimation method that exploits available a priori knowledge from other sensors to compensate dominant flow in order to facilitate estimation of the remaining part of motion using a classical optical flow method. We focus also on the second preliminary step of our detection approach: FOE estimation as its location & required as an input of our algorithm. We have studied and compared different FOE estimation methods. Finally, we have different decision strategies to detect more precisely obstacles 
